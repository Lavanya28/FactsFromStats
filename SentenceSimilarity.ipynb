{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Articles.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cdc536d10d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Articles.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0muncleanedarticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Articles.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "a = open(\"Articles.pickle\",'rb')\n",
    "uncleanedarticles = pickle.load(a)\n",
    "articles=[]\n",
    "text=\"\"\n",
    "title=\"\"\n",
    "titles=[]\n",
    "sources=[]\n",
    "for i in uncleanedarticles:\n",
    "    source,url,title,text = i[0],i[1],i[2],i[3]\n",
    "    sources.append(source)\n",
    "    text = text.replace('\\n\\n','\\n')\n",
    "    if text ==\"\":\n",
    "        continue\n",
    "#     article = i.split('\\n\\n\\n\\n\\n')\n",
    "#     text = article[1].strip('\\n').strip(' ')\n",
    "    if text.count('\\n') <10:\n",
    "        continue\n",
    "    if text.count('https://') >10:\n",
    "        continue   \n",
    "#     title = article[0].strip('\\n').strip(' ')\n",
    "    title = title.strip(' ')\n",
    "    if title in titles:\n",
    "        continue\n",
    "    titles.append(title)\n",
    "    articles.append([source,url,title,text])\n",
    "a.close()\n",
    "# print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-14be479a47f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0marticleslen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "ps = SnowballStemmer('english')\n",
    "wordRepArticles =[]\n",
    "idf={}\n",
    "\n",
    "articleslen=[]\n",
    "for i in articles:\n",
    "    title,text = i[2],i[3]\n",
    "    text = text.replace('\\\\','');text = text.replace('.\\n','\\n');text = text.replace('\\n\\n','\\n')\n",
    "    articledist={}\n",
    "    sentencelis=[]\n",
    "    count=0\n",
    "    for i in (text+'\\n'+title).split('\\n'):\n",
    "        if i ==title:\n",
    "            mult =2\n",
    "        else:\n",
    "            mult =1\n",
    "        linedist={}\n",
    "        v = i.split(' ')\n",
    "        if len(v)<4:\n",
    "            continue\n",
    "        for word in v:\n",
    "            if word not in stopwords.words('english'):\n",
    "                word = word.replace('(','');word = word.replace(')','')\n",
    "                word = word.replace('\"','');word = word.replace(',','');word = word.strip(' ')\n",
    "                stemmed = ps.stem(word)\n",
    "                if stemmed in articledist.keys():\n",
    "                    articledist[stemmed]+=mult\n",
    "                else:\n",
    "                    articledist[stemmed]=mult\n",
    "                if stemmed in linedist.keys():\n",
    "                    linedist[stemmed]+=1\n",
    "                else:\n",
    "                    linedist[stemmed]=1\n",
    "            count+=1\n",
    "        sentencelis.append([i,linedist])\n",
    "    for words in articledist.keys():   \n",
    "        if words in idf.keys():\n",
    "            idf[words]+=1\n",
    "        else:\n",
    "            idf[words]=1\n",
    "    wordRepArticles.append([title,articledist,sentencelis])\n",
    "    articleslen.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(wordRepArticles))\n",
    "# print(wordRepArticles[0])\n",
    "print(len(articles))\n",
    "print(len(uncleanedarticles))\n",
    "fileObject = open('./mysite/cleanedarticles.pickle','wb')\n",
    "pickle.dump(articles,fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "n = len(articles)\n",
    "avgArticleLen = sum(articleslen)*1.0/n\n",
    "match = [[0 for i in range(n)] for i in range(n)]\n",
    "for i in range(n):\n",
    "    mainarticle = wordRepArticles[i]\n",
    "    mainlength = articleslen[i]\n",
    "    for j in range(i+1,n):\n",
    "        comparearticle = wordRepArticles[j]\n",
    "        comparelength = articleslen[j]\n",
    "        commonwords = list(set(mainarticle[1].keys()) & set(comparearticle[1].keys())) \n",
    "        value = 0\n",
    "        for word in commonwords:\n",
    "            mainvalue = (1+math.log(mainarticle[1][word]))*1.0/(math.log(1+mainlength/avgArticleLen))\n",
    "            compvalue = (1+math.log(comparearticle[1][word]))*1.0/(math.log(1+comparelength/avgArticleLen))\n",
    "            value += mainvalue*compvalue* math.log(1+n/idf[word])\n",
    "        match[i][j] = value\n",
    "        match[j][i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topmatches = [[] for i in range(n)]\n",
    "matches = 3\n",
    "for i in range(n):\n",
    "    vals = [[match[i][j],j] for j in range(n)]\n",
    "    toparticle = sorted(vals,reverse=True)[:matches]\n",
    "    print(\"Main title: \"+ articles[i][2])\n",
    "    for j in range(matches):\n",
    "        topmatches[i].append(toparticle[j])\n",
    "        print(str(j)+\"/\" + str(matches)+ \": \" + articles[toparticle[j][1]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avgArticleLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentmatch = []\n",
    "for i in range(n):\n",
    "    mainarticle = wordRepArticles[i]\n",
    "    rep=[]\n",
    "    for j in range(matches):\n",
    "        comparearticle = wordRepArticles[topmatches[i][j][1]]\n",
    "        bestmatchSentence =[]\n",
    "        for k in range(len(mainarticle[2])):\n",
    "            val = -1\n",
    "            msentence = mainarticle[2][k][1]\n",
    "            mainlength = len(mainarticle[2][k][0].split(' '))\n",
    "            for l in range(len(comparearticle[2])):\n",
    "                csentence = comparearticle[2][l][1]\n",
    "                comparelength = len(comparearticle[2][l][0].split(' '))\n",
    "                commonwords = list(set(msentence.keys()) & set(csentence.keys())) \n",
    "                value = 0\n",
    "                for word in commonwords:\n",
    "                    mainvalue = (1+math.log(msentence[word]))*1.0/(math.log(1+mainlength/avgArticleLen))\n",
    "                    compvalue = (1+math.log(csentence[word]))*1.0/(math.log(1+comparelength/avgArticleLen))\n",
    "                    value += mainvalue*compvalue* math.log(1+n/idf[word])\n",
    "                if value>val:\n",
    "                    val = value\n",
    "                    ind = l\n",
    "            bestmatchSentence.append([ind,val,comparearticle[2][ind][0]])\n",
    "        rep.append(bestmatchSentence)\n",
    "    sentmatch.append(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer() \n",
    "\n",
    "reliability = [9.9 for i in range(len(sources))]\n",
    "articleaggreeement=[]\n",
    "for i in range(n):\n",
    "    articleval =0\n",
    "    mainarticle = wordRepArticles[i]\n",
    "    for j in range(matches):\n",
    "        val=0\n",
    "        comparticle=wordRepArticles[topmatches[i][j][1]]\n",
    "        for k in range(len(mainarticle[2])):\n",
    "            sent1 = analyzer.polarity_scores(mainarticle[2][k][0])['compound']\n",
    "            sent2 = analyzer.polarity_scores(sentmatch[i][j][k][2])['compound']\n",
    "            val +=sent1*sent2\n",
    "        val /= len(mainarticle[2])\n",
    "        articleval +=val*reliability[sources.index(articles[topmatches[i][j][1]][0])]\n",
    "    articleaggreeement.append(articleval/matches)\n",
    "    articles[i].append(articleval/matches)\n",
    "    articles[i].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fileObject = open('cleanedarticles.pickle','wb')\n",
    "# pickle.dump(articles,fileObject)\n",
    "# fileObject.close()\n",
    "print(articles[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcerelia=[]\n",
    "for i in range(len(sources)):\n",
    "    sourcerelia.append([sources[i],reliability[i]])\n",
    "fileObject = open('./mysite/sourcereliability.pickle','wb')\n",
    "pickle.dump(sourcerelia,fileObject)\n",
    "fileObject.close()\n",
    "# print(sourcerelia[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesimilarity=[]\n",
    "for i in range(n):\n",
    "    for j in range(len(topmatches[i])):\n",
    "#         articlesimilarity.append([articles[i][2],articles[topmatches[i][j][1]][2]])\n",
    "        articlesimilarity.append([i,topmatches[i][j][1],topmatches[i][j][0]])\n",
    "fileObject = open('./mysite/articlessimilarity.pickle','wb')\n",
    "pickle.dump(articlesimilarity,fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articlesimilarity[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileObject = open('./mysite/articlessimilarity.pickle','rb')\n",
    "# a=pickle.load(fileObject)\n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
